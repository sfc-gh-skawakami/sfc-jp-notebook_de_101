{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8ad35e-7d1c-4aee-9f54-6949d9aa3afe",
   "metadata": {
    "collapsed": false,
    "name": "Intro1"
   },
   "source": [
    "## Avalanche (架空のウィンタースポーツ用品会社)\n",
    "\n",
    "![IMAGE](https://lh3.googleusercontent.com/pw/AP1GczPiLLf_4vKqdLeP8xr1GYa4eMa36fYztaHgEmiV94zrOvEsvIcPNWQnr85TIbzktK-fWbx32HgSryaWaaWJZjV35JU8E3krcwepmeQoW19s7UyloBZ4cOMTe-a0zCEz8hRMV1Kg4TM7cyEj13WdVAO2=w960-h540-s-no-gm?authuser=0)\n",
    "\n",
    "\n",
    "Avalancheの注文履歴・出荷データを, [Snowflake 上で動作する pandas](https://docs.snowflake.com/en/developer-guide/snowpark/python/pandas-on-snowflake) を使って分析します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "# Snowpark Pandas API\n",
    "import modin.pandas as spd\n",
    "# Import the Snowpark pandas plugin for modin\n",
    "import snowflake.snowpark.modin.plugin\n",
    "import streamlit as st\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.context import get_active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "snowpark_session"
   },
   "outputs": [],
   "source": [
    "# Snowflake のアクティブなセッション（現在接続中のセッション）を取得する\n",
    "# これによって、以降の処理で Snowflake に対して SQL 実行やデータ操作ができるようになる\n",
    "session = get_active_session()\n",
    "\n",
    "# セッションに「クエリタグ (query tag)」を設定する\n",
    "# クエリタグとは、Snowflake 上で実行した SQL クエリに「ラベル」を付ける仕組み\n",
    "# これにより、モニタリングやトラブルシューティングで\n",
    "# 「どのアプリから来たクエリか」「どのハンズオン教材からの実行か」などを追跡できる\n",
    "session.query_tag = {\n",
    "    \"origin\": \"sf_devrel\",          # クエリの発行元（ここでは Snowflake Developer Relations の意味）\n",
    "    \"name\": \"de_100_vhol\",          # このハンズオンや演習の名前\n",
    "    \"version\": {                    # バージョン情報\n",
    "        \"major\": 1,\n",
    "        \"minor\": 0\n",
    "    },\n",
    "    \"attributes\": {                 # 追加の属性情報（カスタムラベルのようなもの）\n",
    "        \"is_quickstart\": 1,         # Quickstart チュートリアルからの実行であることを示す\n",
    "        \"source\": \"notebook\",       # Jupyter Notebook や Snowflake Notebook からの実行であることを示す\n",
    "        \"vignette\": \"snowpark_pandas\"  # この教材のシナリオ名（Snowpark + pandas のハンズオンであること）\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# get_active_session() → すでに開いている Snowflake との接続を取ってくる関数。\n",
    "# query_tag → Snowflake に「このクエリは何のために動いたのか」を残せる便利なメタ情報。運用や監査で役立ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109ca7b-e6e2-4fc2-bc33-801e1bebf29f",
   "metadata": {
    "collapsed": false,
    "name": "to_do_1"
   },
   "source": [
    "### TODO: ダウンロードした, 出荷データ(shipping-logs.csv)を Notebooks ワークスペースに読み込む\n",
    "- 画面左側の[➕]ボタンからファイルをアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98d82e-5259-4e73-aabe-79bbb2b59c38",
   "metadata": {
    "language": "python",
    "name": "shipping_logs"
   },
   "outputs": [],
   "source": [
    "# Snowpark pandas（spd）を使って CSV ファイルを読み込む\n",
    "# 'shipping-logs.csv' という名前のCSVファイルを対象にしている\n",
    "# CSVの中に 'shipping_date' という列があり、それを日付型（datetime型）として扱うよう指定している\n",
    "shipping_logs_mdf = spd.read_csv(\n",
    "    'shipping-logs.csv',        # 読み込むCSVファイルの名前\n",
    "    parse_dates=['Shipping Date']  # この列を「文字列」ではなく「日付」として読み込む\n",
    ")\n",
    "\n",
    "# 読み込んだデータ（shipping_logs_mdf）を表示する\n",
    "# shipping_logs_mdf は pandas.DataFrame と同じように扱えるオブジェクト\n",
    "shipping_logs_mdf\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# ****\n",
    "#\n",
    "# parse_dates=['Shipping Date'] → もし指定しないと \"2025-08-17\" のような日付も文字列（ただのテキスト）として扱われる。\n",
    "# → ここで指定することで「日付」として認識され、後で「日付ごとの集計」や「期間でフィルタ」などが簡単にできる。\n",
    "#\n",
    "# shipping_logs_mdf → 読み込んだデータを DataFrame 形式で保持する変数。\n",
    "# shipping_logs_mdf → 変数名の末尾 mdf は「modin dataframe（＝Snowpark pandasのDataFrame）」の略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0fbae-e6ef-462e-8ddb-e575b59e3b74",
   "metadata": {
    "collapsed": false,
    "name": "to_do_2"
   },
   "source": [
    "### TODO: ダウンロードした, 注文履歴データ(order-history.csv)を Notebooks ワークスペースに読み込む\n",
    "- 画面左側の[➕]ボタンからファイルをアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be293e-956d-4781-be26-bbbdd94afc97",
   "metadata": {
    "language": "python",
    "name": "order_history"
   },
   "outputs": [],
   "source": [
    "# Snowpark pandas（spd）を使って CSV ファイルを読み込む\n",
    "# 'order-history.csv' という名前のCSVファイルを対象にしている\n",
    "# CSVの中に 'Date' という列があり、それを日付型（datetime型）として扱うように指定している\n",
    "order_history_mdf = spd.read_csv(\n",
    "    'order-history.csv',   # 読み込むCSVファイルの名前\n",
    "    parse_dates=['Ordered Date']   # 'Date' 列を文字列ではなく「日付」として扱う\n",
    ")\n",
    "\n",
    "# 読み込んだデータ（order_history_mdf）を表示する\n",
    "# order_history_mdf は pandas.DataFrame と同じように扱えるオブジェクト\n",
    "order_history_mdf\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# ****\n",
    "#\n",
    "# parse_dates=['Date'] → 「注文日（Date）」の列を日付型にしておくと、後で「月ごとの集計」「特定の期間の抽出」などが簡単にできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87b19c-07ca-4bec-8779-d6396126179b",
   "metadata": {
    "language": "python",
    "name": "rename_columns"
   },
   "outputs": [],
   "source": [
    "# order_history_mdf の列名を分かりやすく変更する\n",
    "# ****(columns={...}) で「元の列名 : 新しい列名」を指定する\n",
    "\n",
    "order_history_mdf = order_history_mdf.rename(columns = {\n",
    "    'Order ID': 'order_id',              # 注文ID → order_id\n",
    "    'Customer ID': 'customer_id',        # 顧客ID → customer_id\n",
    "    'Product ID': 'product_id',          # 商品ID → product_id\n",
    "    'Product Name': 'product_name',      # 商品名 → product_name\n",
    "    'Quantity Ordered': 'quantity_ordered',  # 注文数 → quantity_ordered\n",
    "    'Price': 'price',                    # 単価 → price\n",
    "    'Total Price': 'total_price',        # 合計金額 → total_price\n",
    "    'Ordered Date': 'date'                       # 日付 → date\n",
    "})\n",
    "\n",
    "# 列名が正しく変更されたかを確認する\n",
    "order_history_mdf.columns\n",
    "\n",
    "# ✨ ポイント\n",
    "#--------------\n",
    "# ****(columns={...}) → 辞書形式（キー: 値）で「古い名前 → 新しい名前」に変換する。\n",
    "#\n",
    "# スネークケース（例: order_id）は Python で一般的な書き方で、プログラムで扱いやすい。\n",
    "#\n",
    "# この処理をしておくと、後でコードを書くときに order_history_mdf[\"order_id\"] のように呼び出しやすくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9f31f-f328-4b33-b567-fb879efa0f9e",
   "metadata": {
    "collapsed": false,
    "name": "remove_dollar"
   },
   "source": [
    "### 価格カラムから $ 記号を取り除いて整理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b710b6-cee7-478e-9870-c2dd5c3119f8",
   "metadata": {
    "language": "python",
    "name": "clean_price_func"
   },
   "outputs": [],
   "source": [
    "# 文字列で表現された価格（例: \"$19.99\"）を数値に変換する関数\n",
    "def clean_price(price_str):\n",
    "    # 価格の文字列から \"$\" 記号を取り除き、前後の余分な空白も削除する\n",
    "    # 例: \" $19.99 \" → \"19.99\"\n",
    "    cleaned = price_str.replace('$', '').strip()\n",
    "    \n",
    "    # 文字列になっている数値を float型（小数点を持つ数値）に変換する\n",
    "    # 例: \"19.99\" → 19.99\n",
    "    return float(cleaned)\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# **** → $ を空文字に置き換えて削除する。\n",
    "#\n",
    "# .strip() → 文字列の前後にある余計なスペースや改行を削除する。\n",
    "#\n",
    "# float() → 文字列を「実数」に変換する。計算で使えるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2da873-f6ed-4e62-9518-845dc99e5ac9",
   "metadata": {
    "language": "python",
    "name": "clean_up_price_values"
   },
   "outputs": [],
   "source": [
    "# ---- 価格カラムを数値に変換する処理 ----\n",
    "\n",
    "# 'price' 列に対して clean_price 関数を適用する\n",
    "# これにより \"$19.99\" のような文字列が 19.99 という float型の数値になる\n",
    "order_history_mdf['price'] = order_history_mdf['price'].apply(clean_price)\n",
    "\n",
    "# 'total_price' 列に対しても同じく clean_price を適用する\n",
    "# これで合計金額も数値として扱えるようになる\n",
    "order_history_mdf['total_price'] = order_history_mdf['total_price'].apply(clean_price)\n",
    "\n",
    "\n",
    "# ---- 変換後のデータ型を確認する処理 ----\n",
    "\n",
    "# price 列のデータ型を表示（float になっていればOK）\n",
    "print(\"\\nPrice column data type:\", order_history_mdf['price'].dtype)\n",
    "\n",
    "# total_price 列のデータ型を表示（こちらも float になっていればOK）\n",
    "print(\"Total price column data type:\", order_history_mdf['total_price'].dtype)\n",
    "\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# .apply(clean_price) → 各行の値に対して clean_price 関数を実行する。\n",
    "#\n",
    "# データ型の確認 (dtype)\n",
    "# 変換前 → object（文字列）\n",
    "# 変換後 → float64（小数点付き数値）\n",
    "#\n",
    "# こうしておくことで、後から「平均」「合計」「グラフ化」などの数値演算ができるようになる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae513f-b06b-4a08-9bf5-e97db9a98eed",
   "metadata": {
    "language": "python",
    "name": "check_clean_up_price_values"
   },
   "outputs": [],
   "source": [
    "# 実際に $ が消えて数値化されているかを表で確認\n",
    "order_history_mdf.head()\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# .head() → データフレームの最初の5行を表示して内容を確認する\n",
    "# 読み込みや列名変更、数値変換が正しくできているかチェックするのに便利"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9aaed-ab1c-482f-b87e-fa4c3955fdac",
   "metadata": {
    "collapsed": false,
    "name": "join_order_shipping"
   },
   "source": [
    "### 製品ごとの注文数を計算する：order_history と shipping_logs を結合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cfffd-e1fc-4fdb-bc42-dcfb99eb1bdd",
   "metadata": {
    "language": "python",
    "name": "rename_columns2"
   },
   "outputs": [],
   "source": [
    "# order_history_mdf の列名を分かりやすく変更する\n",
    "# ****(columns={...}) で「元の列名 : 新しい列名」を指定する\n",
    "\n",
    "shipping_logs_mdf = shipping_logs_mdf.rename(columns = {\n",
    "    'Order ID': 'order_id',              # 注文ID → order_id\n",
    "    'Shipping Date': 'shipping_date',    # 発送日 → shipping_date\n",
    "    'Carrier': 'carrier',                # 配送業者 → carrier\n",
    "    'Tracking Number': 'trucking_number',# 追跡番号 → trucking_number\n",
    "    'Latitude': 'latitude',              # 緯度 → lattitude\n",
    "    'Longitude': 'longitude',            # 経度 → longitude\n",
    "    'Shipping Status': 'status'         # ステータス → status\n",
    "})\n",
    "\n",
    "# 列名が正しく変更されたかを確認する\n",
    "shipping_logs_mdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec8cd02-16c1-4999-8870-da25c1281f27",
   "metadata": {
    "language": "python",
    "name": "join_tables"
   },
   "outputs": [],
   "source": [
    "# ---- 注文データと出荷データを結合 ----\n",
    "\n",
    "# order_history_mdf（注文データ）と shipping_logs_mdf（出荷データ）を\n",
    "# 'order_id' 列をキーにして結合（マージ）する\n",
    "\n",
    "order_shipping_mdf = spd.merge(\n",
    "    order_history_mdf,      # 左側のデータフレーム（注文履歴）\n",
    "    shipping_logs_mdf,      # 右側のデータフレーム（出荷ログ）\n",
    "    on='order_id',          # 結合キーとなる列\n",
    "    how='inner'             # 内部結合（両方に存在するデータのみ）\n",
    ")\n",
    "\n",
    "# 結合後のデータフレームの先頭5行を表示して確認\n",
    "order_shipping_mdf.head(5)\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# spd.****() → pandas の **** と同じように使えるが、Snowpark pandas 上で動作する\n",
    "#\n",
    "# on='order_id' → 「注文ID」を使ってデータを紐付ける\n",
    "#\n",
    "# how='****'：\n",
    "#     両方のテーブルに存在する注文だけを残す\n",
    "#     片方だけにある注文は削除される\n",
    "#     結合すると、注文情報と出荷情報が 1行にまとめられる ので分析しやすくなる\n",
    "#         例) 「注文から出荷までの日数」や「商品ごとの売上と出荷状況」\n",
    "#\n",
    "# 外部結合（how='left' や how='right'）を使うと、片方にしかないデータも残せる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb33443-f7c3-41a5-acfe-6cbe634ab990",
   "metadata": {
    "language": "python",
    "name": "product_order_counts"
   },
   "outputs": [],
   "source": [
    "# ---- 商品ごとの注文件数を集計 ----\n",
    "\n",
    "# 'product_name' 列でグループ化して、注文件数を数える\n",
    "# ****() は各グループの行数（＝注文数）をカウントする\n",
    "# reset_index(name='order_count') で結果をデータフレーム形式に戻し、列名を 'order_count' に設定\n",
    "product_counts_mdf = order_shipping_mdf.groupby('product_name').size().reset_index(name='order_count')\n",
    "\n",
    "# ---- 注文件数の多い順に並べ替え ----\n",
    "\n",
    "# sort_values() で 'order_count' 列を降順（ascending=False）に並べる\n",
    "product_counts_mdf = product_counts_mdf.sort_values('order_count', ascending=False)\n",
    "\n",
    "# ---- 結果を表示 ----\n",
    "print(\"\\nProduct Order Counts:\")\n",
    "st.dataframe(product_counts_mdf)\n",
    "\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# ****('product_name') → 同じ商品ごとにまとめる \n",
    "#\n",
    "# .****() → 各商品の注文数をカウント\n",
    "#\n",
    "# .reset_index(name='order_count') → 集計結果をデータフレームとして整形し、列名を order_count に変更\n",
    "#\n",
    "# .sort_values(..., ascending=False) → 注文件数の多い順に並べ替える\n",
    "# --------------\n",
    "# これにより、どの商品が人気か（注文が多いか）が一目でわかる\n",
    "# もし上位5商品だけ見たい場合は product_counts_mdf.head(5) と書くと便利\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cd23f-deb0-41b3-91f5-7294a6ee6267",
   "metadata": {
    "collapsed": false,
    "name": "pivot"
   },
   "source": [
    "### 注文の配送ステータスごとにピボットする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803b11b-55e1-4b44-869f-e67d3c95639c",
   "metadata": {
    "language": "python",
    "name": "product_status_pivot"
   },
   "outputs": [],
   "source": [
    "# ---- 商品ごとの注文ステータス別集計 ----\n",
    "\n",
    "# **** を使って集計\n",
    "# index='product_name' → 行に商品名を設定\n",
    "# columns='status' → 列に注文ステータス（例: shipped, pending, cancelled）を設定\n",
    "# values='order_id' → 注文IDを数える対象にする\n",
    "# aggfunc='count' → 各セルに注文数をカウント\n",
    "# fill_value=0 → データがない場合は 0 を埋める\n",
    "product_status_pivot_mdf = order_shipping_mdf.pivot_table(\n",
    "    index='product_name',\n",
    "    columns='status',\n",
    "    values='order_id',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# ---- 合計注文数の列を追加 ----\n",
    "\n",
    "# 行ごとの合計を計算して 'Total_Orders' 列として追加\n",
    "product_status_pivot_mdf['Total_Orders'] = product_status_pivot_mdf.sum(axis=1)\n",
    "\n",
    "# ---- 合計注文数の多い順に並べ替え ----\n",
    "\n",
    "product_status_pivot_mdf = product_status_pivot_mdf.sort_values('Total_Orders', ascending=False)\n",
    "\n",
    "# ---- 結果を表示 ----\n",
    "print(\"\\nProduct Orders by Status:\")\n",
    "st.dataframe(product_status_pivot_mdf)\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# **** → 行・列を指定して集計表（ピボットテーブル）を作る\n",
    "\n",
    "# aggfunc='count' → 注文数をカウント\n",
    "\n",
    "# sum(axis=1) → 行方向の合計を計算して「合計注文数」を追加\n",
    "\n",
    "# この表を作ると、\n",
    "# 商品ごとのステータス別の注文数が一目でわかる\n",
    "# 合計注文数で人気商品をすぐ把握できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8654c-416d-4279-8c21-b53987491ca8",
   "metadata": {
    "collapsed": false,
    "name": "intro2"
   },
   "source": [
    "## Avalanche社は、各製品に対する顧客レビューについても理解したいと考えています。  \n",
    "\n",
    "![IMAGE](https://lh3.googleusercontent.com/pw/AP1GczMuP-pHWhjNDtQwRpMYm0FKey9xlDfRMvcSa6HhxnJrhG-oCs6ydlOhpCvR5VcNDjbFNRir_H4XsFaay-lehwzRV1pgKoB9DjJ31SduUCD2F1gwmZgG4SAM6vNseULS3tYZoW7taYzTW-gc5Lt-4gu3=w960-h540-s-no-gm?authuser=0)\n",
    "\n",
    "\n",
    "\n",
    "この分析を [Snowpark DataFrame API](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes) を使って実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a31fe-3a77-4966-bfef-c29cb3c1fe1a",
   "metadata": {
    "language": "sql",
    "name": "create_snowflake_objects"
   },
   "outputs": [],
   "source": [
    "-- ---- データベースとスキーマの作成（Snowsight UI で実行する場合） ----\n",
    "-- CREATE OR REPLACE DATABASE avalanche_db;\n",
    "-- CREATE OR REPLACE SCHEMA avalanche_schema;\n",
    "\n",
    "-- 既存データベースを使用する\n",
    "USE DATABASE avalanche_db;\n",
    "\n",
    "-- 既存スキーマを使用する\n",
    "USE SCHEMA avalanche_schema;\n",
    "\n",
    "\n",
    "-- ---- ファイルを格納するステージ（Stage）の作成 ----\n",
    "-- Stage とは、Snowflake にデータを取り込む前に一時的にファイルを置いておく場所です\n",
    "CREATE OR REPLACE STAGE avalanche_stage\n",
    "  URL = 's3://sfquickstarts/misc/avalanche/csv/'  -- S3 バケットの場所を指定\n",
    "  DIRECTORY = (ENABLE = TRUE AUTO_REFRESH = TRUE); -- ディレクトリ構造を有効化、自動更新ON\n",
    "\n",
    "-- Stage 内のファイル一覧を確認\n",
    "ls @avalanche_stage;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6a184-8c2f-4d54-a606-fd3f2acc1d22",
   "metadata": {
    "collapsed": false,
    "name": "load_reviews"
   },
   "source": [
    "### 顧客レビューを Snowflake のテーブルに読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903c319-f55a-4773-8e58-daaec82f72d3",
   "metadata": {
    "language": "sql",
    "name": "create_customer_reviews_table"
   },
   "outputs": [],
   "source": [
    "-- ---- テーブルの作成 ----\n",
    "-- customer_reviews という名前のテーブルを作成\n",
    "-- 商品名、レビュー日、レビュー本文、感情スコアを保存する\n",
    "CREATE OR REPLACE TABLE customer_reviews (\n",
    "    product VARCHAR,          -- 商品名（文字列）\n",
    "    date DATE,                -- レビュー日（DATE型）\n",
    "    summary TEXT,             -- レビュー本文（TEXT型）\n",
    "    sentiment_score FLOAT     -- 感情スコア（数値、小数点）\n",
    ");\n",
    "\n",
    "\n",
    "-- ---- CSV ファイルからデータをテーブルにロード ----\n",
    "COPY INTO customer_reviews\n",
    "FROM @avalanche_stage/customer_reviews.csv   -- 先ほど作成した Stage 内の CSV を指定\n",
    "FILE_FORMAT = (\n",
    "    TYPE = CSV,                               -- CSV形式のファイル\n",
    "    FIELD_DELIMITER = ',',                     -- カラム区切り文字はカンマ\n",
    "    SKIP_HEADER = 1,                           -- 1行目はヘッダーなのでスキップ\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',       -- 値が \" \" で囲まれている場合に対応\n",
    "    TRIM_SPACE = TRUE,                         -- 前後の空白を削除\n",
    "    NULL_IF = ('NULL', 'null'),               -- \"NULL\" または \"null\" は NULL として扱う\n",
    "    EMPTY_FIELD_AS_NULL = TRUE                -- 空欄も NULL として扱う\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558d8e0-c499-4ae5-9ca0-85497fedc71a",
   "metadata": {
    "language": "python",
    "name": "load_customer_reviews"
   },
   "outputs": [],
   "source": [
    "# ---- Snowflake テーブルを Snowpark DataFrame として読み込む ----\n",
    "\n",
    "# 'customer_reviews' テーブルを Snowpark DataFrame として取得\n",
    "customer_reviews_sdf = session.table('customer_reviews')\n",
    "\n",
    "# 取得した Snowpark DataFrame の内容を確認\n",
    "customer_reviews_sdf\n",
    "\n",
    "\n",
    "# ✨ ポイント\n",
    "# --------------\n",
    "# session.table('table_name')\n",
    "#   Snowflake 上の既存テーブルを Snowpark DataFrame として扱う\n",
    "#   pandas の DataFrame に似ているが、実際のデータは Snowflake にあり、クエリ実行時に必要な部分だけ取得する\n",
    "#\n",
    "# customer_reviews_sdf\n",
    "#   この変数に Snowpark DataFrame が格納される\n",
    "#   データの操作（フィルタリング、集計、結合など）を Snowflake 側で効率的に実行可能\n",
    "\n",
    "# 💡 補足\n",
    "# --------------\n",
    "# Snowpark DataFrame は Lazy Evaluation（遅延評価） です\n",
    "#   customer_reviews_sdf を定義しただけではまだデータは取得されない\n",
    "#   データを確認したい場合は .show() や .to_pandas() などで明示的に取得する必要があります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eddc5a-cd65-4ceb-a20e-057eade19152",
   "metadata": {
    "language": "python",
    "name": "drop_sentiment"
   },
   "outputs": [],
   "source": [
    "product_sentiment_sdf = customer_reviews_sdf.group_by('PRODUCT') \\\n",
    "    .agg(F.round(F.avg('SENTIMENT_SCORE'),2).alias('AVG_SENTIMENT_SCORE')) \\\n",
    "    .sort(F.col('AVG_SENTIMENT_SCORE').desc())\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nAverage Sentiment Scores by Product:\")\n",
    "product_sentiment_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125cda9-34ed-421b-ab71-b92b5ba546a4",
   "metadata": {
    "collapsed": false,
    "name": "visualize_md"
   },
   "source": [
    "## 📊 データの可視化\n",
    "\n",
    "[Altair](https://altair-viz.github.io/)を使用して、データ分布をヒストグラムとして簡単に可視化できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01df47-4480-4069-adca-954fb3bb8fc0",
   "metadata": {
    "language": "python",
    "name": "visualize"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# 追加したパッケージ\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = customer_reviews_sdf.to_pandas()\n",
    "chart = alt.Chart(pdf, title='評価分布').mark_bar().encode(\n",
    "    alt.X(\"SENTIMENT_SCORE\", bin=alt.Bin(step=0.5)),\n",
    "    y='count()'\n",
    ")\n",
    "\n",
    "st.altair_chart(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987baa63-1214-40f2-b323-fdf963c63876",
   "metadata": {
    "collapsed": false,
    "name": "plotting_md"
   },
   "source": [
    "チャートをカスタマイズして、カーネル密度推定（KDE）と中央値をプロットしたいとします。matplotlibを使用して価格分布をプロットできます。`.plot`コマンドは内部的に`scipy`を使用してKDEプロファイルを計算することに注意してください。これは、このチュートリアルの前半でパッケージとして追加したものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6993a-e1eb-49b1-bfab-f32cc9350eb9",
   "metadata": {
    "language": "python",
    "name": "plotting"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,3))\n",
    "plt.tick_params(left = False, right = False , labelleft = False) \n",
    "\n",
    "price = order_history_mdf[\"price\"]\n",
    "price.plot(kind = \"hist\", density = True, bins = 15)\n",
    "price.plot(kind=\"kde\", color='#c44e52')\n",
    "\n",
    "\n",
    "# パーセンタイルを計算\n",
    "median = price.median()\n",
    "ax.axvline(median,0, color='#dd8452', ls='--')\n",
    "ax.text(median,0.8, f'Median: {median:.2f}  ',\n",
    "        ha='right', va='center', color='#dd8452', transform=ax.get_xaxis_transform())\n",
    "\n",
    "# チャートを美しくする\n",
    "plt.style.use(\"bmh\")\n",
    "plt.title(\"Price Distribution\")\n",
    "plt.xlabel(\"Price (Binned)\")\n",
    "left, right = plt.xlim()   \n",
    "plt.xlim((0, right))  \n",
    "# 目盛りと軸線を削除\n",
    "ax.tick_params(left = False, bottom = False)\n",
    "for ax, spine in ax.spines.items():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa1fbd-5483-4e60-a893-f92fab942ad6",
   "metadata": {
    "collapsed": false,
    "name": "cell_reference_md"
   },
   "source": [
    "## サブクエリ/セル間の参照\n",
    "\n",
    "セルに名前をつけて、後続のセルでその出力を参照することができます\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8bf3e-25a1-458d-8f59-169f4a1ec220",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "Jinjaを利用して別のSQLセルからSQLテーブルを参照することで、CTEを簡素化することができます。\n",
    "\n",
    "```sql\n",
    "SELECT * FROM {{cell}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afa98b-8abf-4888-9029-cf197cc05393",
   "metadata": {
    "language": "sql",
    "name": "subqueries"
   },
   "outputs": [],
   "source": [
    "select \n",
    "    product, \n",
    "    avg(sentiment_score) as avg_score, \n",
    "    min(sentiment_score) as min_score, \n",
    "    max(sentiment_score) as max_score\n",
    "from customer_reviews\n",
    "group by all;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d7660-8ecc-4b3e-89e3-619907a8d966",
   "metadata": {
    "language": "sql",
    "name": "subqueries2"
   },
   "outputs": [],
   "source": [
    "select * from {{subqueries}}\n",
    "WHERE avg_score > 0.5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fde296-4fa7-49bf-add4-79cafe529d48",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "SQL結果にPythonから直接アクセスし、結果をpandas DataFrameに変換できます。🐼\n",
    "\n",
    "```python\n",
    "# SQLセルの出力をSnowpark DataFrameとしてアクセス\n",
    "my_snowpark_df = sql_querying.to_df()\n",
    "``` \n",
    "\n",
    "```python\n",
    "# SQLセルの出力をpandas DataFrameに変換\n",
    "my_df = sql_querying.to_pandas()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4677009-d9e0-4690-82ab-8a7196026fae",
   "metadata": {
    "language": "python",
    "name": "cell_reference"
   },
   "outputs": [],
   "source": [
    "my_df = subqueries2.to_df()\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58c54d-15fa-4fba-a124-1a56a7947b29",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "## ステージパッケージの追加\n",
    "使用したいPythonパッケージがAnacondaで利用できない場合は、パッケージをステージにアップロードし、ステージからインポートすることができます。ここでは、カスタムパッケージをノートブックにインポートする簡単な例を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e372-2354-48c9-8a27-f388ab75ffef",
   "metadata": {
    "language": "sql",
    "name": "git_integration"
   },
   "outputs": [],
   "source": [
    "-- ステージの作成\n",
    "CREATE OR REPLACE STAGE AVALANCHE_DB.AVALANCHE_SCHEMA.FILE DIRECTORY = (ENABLE = TRUE);\n",
    "\n",
    "// Step3: 公開されているGitからスクリプトを取得 //\n",
    "-- Git連携のため、API統合を作成する\n",
    "CREATE OR REPLACE API INTEGRATION git_api_integration\n",
    "  API_PROVIDER = git_https_api\n",
    "  API_ALLOWED_PREFIXES = ('https://github.com/sfc-gh-skawakami/')\n",
    "  ENABLED = TRUE;\n",
    "\n",
    "-- GIT統合の作成\n",
    "CREATE OR REPLACE GIT REPOSITORY GIT_INTEGRATION_FOR_HANDSON\n",
    "  API_INTEGRATION = git_api_integration\n",
    "  ORIGIN = 'https://github.com/sfc-gh-skawakami/sfc-jp-notebook_de_101.git';\n",
    "\n",
    "\n",
    "ALTER GIT REPOSITORY GIT_INTEGRATION_FOR_HANDSON FETCH;\n",
    "\n",
    "-- チェックする\n",
    "ls @GIT_INTEGRATION_FOR_HANDSON/branches/main;\n",
    "\n",
    "-- Githubからファイルを持ってくる\n",
    "COPY FILES INTO @AVALANCHE_DB.AVALANCHE_SCHEMA.FILE FROM @GIT_INTEGRATION_FOR_HANDSON/branches/main/simple.zip;\n",
    "ls @AVALANCHE_DB.AVALANCHE_SCHEMA.FILE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89368f4-1053-481b-a446-d4e1874b8f83",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "`simple.zip`の内容\n",
    "\n",
    "simple/__init__.py\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "def greeting():\n",
    "  return \"Hello world!\"\n",
    "\n",
    "def hi():\n",
    "  st.write(greeting())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c601241-1133-4280-9486-9527a480629b",
   "metadata": {
    "language": "python",
    "name": "stage_packages"
   },
   "outputs": [],
   "source": [
    "import simple\n",
    "\n",
    "simple.hi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4accc8",
   "metadata": {},
   "source": [
    "## プライベートリポジトリからのインポート\n",
    "```\n",
    "コンテナランタイムでのみ使用可能\n",
    "```\n",
    "Pipは、JFrog Artifactoryのようなプライベートソースからのパッケージのインストールを基本認証でサポートしています。ノートブックを外部アクセス統合（External Access Integration）用に設定し、リポジトリにアクセスできるようにします。\n",
    "\n",
    "1. ネットワークルールを作成し、アクセスしたいリポジトリを指定します。例えば、このネットワークルールはJFrogリポジトリを指定しています:\n",
    "```sql\n",
    "CREATE OR REPLACE NETWORK RULE jfrog_network_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = ('<your-repo>.jfrog.io');\n",
    "```\n",
    "2. 外部ネットワーク位置への認証に必要な資格情報を表すシークレットを作成します。\n",
    "```sql\n",
    "CREATE OR REPLACE SECRET jfrog_token\n",
    "  TYPE = GENERIC_STRING\n",
    "  SECRET_STRING = '<your-jfrog-token>';\n",
    "```\n",
    "3. リポジトリへのアクセスを許可する外部アクセス統合を作成します：\n",
    "```sql\n",
    "CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION jfrog_integration\n",
    "  ALLOWED_NETWORK_RULES = (jfrog_network_rule)\n",
    "  ALLOWED_AUTHENTICATION_SECRETS = (jfrog_token)\n",
    "  ENABLED = TRUE;\n",
    "\n",
    "GRANT USAGE ON INTEGRATION jfrog_integration TO ROLE data_scientist;\n",
    "```\n",
    "4. 外部アクセス統合とシークレットをノートブックに関連付けます。\n",
    "```sql\n",
    "ALTER NOTEBOOK my_notebook\n",
    "  SET EXTERNAL_ACCESS_INTEGRATIONS = (jfrog_integration),\n",
    "    SECRETS = ('jfrog_token' = jfrog_token);\n",
    "```\n",
    "5. 外部アクセス設定にアクセスするには、ノートブックの右上にある「ワークシートのその他のアクション」（ノートブックアクションメニュー）を選択します。\n",
    "6. 「ノートブック設定」を選択し、次に「External Access」タブを選択します。\n",
    "7. リポジトリに接続する外部アクセス統合を選択します。\n",
    "ノートブックが再起動します。\n",
    "8. ノートブックが再起動されたら、リポジトリからインストール可能になります\n",
    "```cmd\n",
    "!pip install hello-jfrog --index-url https://<user>:<token>@<your-repo>.jfrog.io/artifactory/api/pypi/test-pypi/simple\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "shinichi.kawakami@snowflake.com",
   "authorId": "3340353333597",
   "authorName": "SKAWAKAMI",
   "lastEditTime": 1756121138188,
   "notebookId": "t2tu5p7ese4jvc6tfj25",
   "sessionId": "72c1613b-19e6-4b6c-b1e1-a56816948f3c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
