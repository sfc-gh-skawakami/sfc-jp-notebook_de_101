{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "t2tu5p7ese4jvc6tfj25",
   "authorId": "3340353333597",
   "authorName": "SKAWAKAMI",
   "authorEmail": "shinichi.kawakami@snowflake.com",
   "sessionId": "72c1613b-19e6-4b6c-b1e1-a56816948f3c",
   "lastEditTime": 1756121138188
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8ad35e-7d1c-4aee-9f54-6949d9aa3afe",
   "metadata": {
    "name": "Intro1",
    "collapsed": false
   },
   "source": "## Avalanche (架空のウィンタースポーツ用品会社)\n\n![IMAGE](https://lh3.googleusercontent.com/pw/AP1GczPiLLf_4vKqdLeP8xr1GYa4eMa36fYztaHgEmiV94zrOvEsvIcPNWQnr85TIbzktK-fWbx32HgSryaWaaWJZjV35JU8E3krcwepmeQoW19s7UyloBZ4cOMTe-a0zCEz8hRMV1Kg4TM7cyEj13WdVAO2=w960-h540-s-no-gm?authuser=0)\n\n\nAvalancheの注文履歴・出荷データを, [Snowflake 上で動作する pandas](https://docs.snowflake.com/en/developer-guide/snowpark/python/pandas-on-snowflake) を使って分析します"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": "# Snowpark Pandas API\nimport modin.pandas as spd\n# Import the Snowpark pandas plugin for modin\nimport snowflake.snowpark.modin.plugin\nimport streamlit as st\n\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.context import get_active_session",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "snowpark_session"
   },
   "source": "# Snowflake のアクティブなセッション（現在接続中のセッション）を取得する\n# これによって、以降の処理で Snowflake に対して SQL 実行やデータ操作ができるようになる\nsession = get_active_session()\n\n# セッションに「クエリタグ (query tag)」を設定する\n# クエリタグとは、Snowflake 上で実行した SQL クエリに「ラベル」を付ける仕組み\n# これにより、モニタリングやトラブルシューティングで\n# 「どのアプリから来たクエリか」「どのハンズオン教材からの実行か」などを追跡できる\nsession.query_tag = {\n    \"origin\": \"sf_devrel\",          # クエリの発行元（ここでは Snowflake Developer Relations の意味）\n    \"name\": \"de_100_vhol\",          # このハンズオンや演習の名前\n    \"version\": {                    # バージョン情報\n        \"major\": 1,\n        \"minor\": 0\n    },\n    \"attributes\": {                 # 追加の属性情報（カスタムラベルのようなもの）\n        \"is_quickstart\": 1,         # Quickstart チュートリアルからの実行であることを示す\n        \"source\": \"notebook\",       # Jupyter Notebook や Snowflake Notebook からの実行であることを示す\n        \"vignette\": \"snowpark_pandas\"  # この教材のシナリオ名（Snowpark + pandas のハンズオンであること）\n    }\n}\n\n\n# ✨ ポイント\n# --------------\n# get_active_session() → すでに開いている Snowflake との接続を取ってくる関数。\n# query_tag → Snowflake に「このクエリは何のために動いたのか」を残せる便利なメタ情報。運用や監査で役立ちます。",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5109ca7b-e6e2-4fc2-bc33-801e1bebf29f",
   "metadata": {
    "name": "to_do_1",
    "collapsed": false
   },
   "source": "### TODO: ダウンロードした, 出荷データ(shipping-logs.csv)を Notebooks ワークスペースに読み込む\n- 画面左側の[➕]ボタンからファイルをアップロード"
  },
  {
   "cell_type": "code",
   "id": "db98d82e-5259-4e73-aabe-79bbb2b59c38",
   "metadata": {
    "language": "python",
    "name": "shipping_logs"
   },
   "outputs": [],
   "source": "# Snowpark pandas（spd）を使って CSV ファイルを読み込む\n# 'shipping-logs.csv' という名前のCSVファイルを対象にしている\n# CSVの中に 'shipping_date' という列があり、それを日付型（datetime型）として扱うよう指定している\nshipping_logs_mdf = spd.read_csv(\n    'shipping-logs.csv',        # 読み込むCSVファイルの名前\n    parse_dates=['Shipping Date']  # この列を「文字列」ではなく「日付」として読み込む\n)\n\n# 読み込んだデータ（shipping_logs_mdf）を表示する\n# shipping_logs_mdf は pandas.DataFrame と同じように扱えるオブジェクト\nshipping_logs_mdf\n\n\n# ✨ ポイント\n# --------------\n# ****\n#\n# parse_dates=['Shipping Date'] → もし指定しないと \"2025-08-17\" のような日付も文字列（ただのテキスト）として扱われる。\n# → ここで指定することで「日付」として認識され、後で「日付ごとの集計」や「期間でフィルタ」などが簡単にできる。\n#\n# shipping_logs_mdf → 読み込んだデータを DataFrame 形式で保持する変数。\n# shipping_logs_mdf → 変数名の末尾 mdf は「modin dataframe（＝Snowpark pandasのDataFrame）」の略",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8be0fbae-e6ef-462e-8ddb-e575b59e3b74",
   "metadata": {
    "name": "to_do_2",
    "collapsed": false
   },
   "source": "### TODO: ダウンロードした, 注文履歴データ(order-history.csv)を Notebooks ワークスペースに読み込む\n- 画面左側の[➕]ボタンからファイルをアップロード"
  },
  {
   "cell_type": "code",
   "id": "17be293e-956d-4781-be26-bbbdd94afc97",
   "metadata": {
    "language": "python",
    "name": "order_history"
   },
   "outputs": [],
   "source": "# Snowpark pandas（spd）を使って CSV ファイルを読み込む\n# 'order-history.csv' という名前のCSVファイルを対象にしている\n# CSVの中に 'Date' という列があり、それを日付型（datetime型）として扱うように指定している\norder_history_mdf = spd.read_csv(\n    'order-history.csv',   # 読み込むCSVファイルの名前\n    parse_dates=['Ordered Date']   # 'Date' 列を文字列ではなく「日付」として扱う\n)\n\n# 読み込んだデータ（order_history_mdf）を表示する\n# order_history_mdf は pandas.DataFrame と同じように扱えるオブジェクト\norder_history_mdf\n\n\n# ✨ ポイント\n# --------------\n# ****\n#\n# parse_dates=['Date'] → 「注文日（Date）」の列を日付型にしておくと、後で「月ごとの集計」「特定の期間の抽出」などが簡単にできる。",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f87b19c-07ca-4bec-8779-d6396126179b",
   "metadata": {
    "language": "python",
    "name": "rename_columns"
   },
   "outputs": [],
   "source": "# order_history_mdf の列名を分かりやすく変更する\n# ****(columns={...}) で「元の列名 : 新しい列名」を指定する\n\norder_history_mdf = order_history_mdf.rename(columns = {\n    'Order ID': 'order_id',              # 注文ID → order_id\n    'Customer ID': 'customer_id',        # 顧客ID → customer_id\n    'Product ID': 'product_id',          # 商品ID → product_id\n    'Product Name': 'product_name',      # 商品名 → product_name\n    'Quantity Ordered': 'quantity_ordered',  # 注文数 → quantity_ordered\n    'Price': 'price',                    # 単価 → price\n    'Total Price': 'total_price',        # 合計金額 → total_price\n    'Ordered Date': 'date'                       # 日付 → date\n})\n\n# 列名が正しく変更されたかを確認する\norder_history_mdf.columns\n\n# ✨ ポイント\n#--------------\n# ****(columns={...}) → 辞書形式（キー: 値）で「古い名前 → 新しい名前」に変換する。\n#\n# スネークケース（例: order_id）は Python で一般的な書き方で、プログラムで扱いやすい。\n#\n# この処理をしておくと、後でコードを書くときに order_history_mdf[\"order_id\"] のように呼び出しやすくなる。",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36e9f31f-f328-4b33-b567-fb879efa0f9e",
   "metadata": {
    "name": "remove_dollar",
    "collapsed": false
   },
   "source": "### 価格カラムから $ 記号を取り除いて整理する"
  },
  {
   "cell_type": "code",
   "id": "05b710b6-cee7-478e-9870-c2dd5c3119f8",
   "metadata": {
    "language": "python",
    "name": "clean_price_func"
   },
   "outputs": [],
   "source": "# 文字列で表現された価格（例: \"$19.99\"）を数値に変換する関数\ndef clean_price(price_str):\n    # 価格の文字列から \"$\" 記号を取り除き、前後の余分な空白も削除する\n    # 例: \" $19.99 \" → \"19.99\"\n    cleaned = price_str.replace('$', '').strip()\n    \n    # 文字列になっている数値を float型（小数点を持つ数値）に変換する\n    # 例: \"19.99\" → 19.99\n    return float(cleaned)\n\n\n# ✨ ポイント\n# --------------\n# **** → $ を空文字に置き換えて削除する。\n#\n# .strip() → 文字列の前後にある余計なスペースや改行を削除する。\n#\n# float() → 文字列を「実数」に変換する。計算で使えるようになる。",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db2da873-f6ed-4e62-9518-845dc99e5ac9",
   "metadata": {
    "language": "python",
    "name": "clean_up_price_values"
   },
   "outputs": [],
   "source": "# ---- 価格カラムを数値に変換する処理 ----\n\n# 'price' 列に対して clean_price 関数を適用する\n# これにより \"$19.99\" のような文字列が 19.99 という float型の数値になる\norder_history_mdf['price'] = order_history_mdf['price'].apply(clean_price)\n\n# 'total_price' 列に対しても同じく clean_price を適用する\n# これで合計金額も数値として扱えるようになる\norder_history_mdf['total_price'] = order_history_mdf['total_price'].apply(clean_price)\n\n\n# ---- 変換後のデータ型を確認する処理 ----\n\n# price 列のデータ型を表示（float になっていればOK）\nprint(\"\\nPrice column data type:\", order_history_mdf['price'].dtype)\n\n# total_price 列のデータ型を表示（こちらも float になっていればOK）\nprint(\"Total price column data type:\", order_history_mdf['total_price'].dtype)\n\n\n\n# ✨ ポイント\n# --------------\n# .apply(clean_price) → 各行の値に対して clean_price 関数を実行する。\n#\n# データ型の確認 (dtype)\n# 変換前 → object（文字列）\n# 変換後 → float64（小数点付き数値）\n#\n# こうしておくことで、後から「平均」「合計」「グラフ化」などの数値演算ができるようになる。\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80ae513f-b06b-4a08-9bf5-e97db9a98eed",
   "metadata": {
    "language": "python",
    "name": "check_clean_up_price_values"
   },
   "outputs": [],
   "source": "# 実際に $ が消えて数値化されているかを表で確認\norder_history_mdf.head()\n\n\n# ✨ ポイント\n# --------------\n# .head() → データフレームの最初の5行を表示して内容を確認する\n# 読み込みや列名変更、数値変換が正しくできているかチェックするのに便利",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7d9aaed-ab1c-482f-b87e-fa4c3955fdac",
   "metadata": {
    "name": "join_order_shipping",
    "collapsed": false
   },
   "source": "### 製品ごとの注文数を計算する：order_history と shipping_logs を結合する"
  },
  {
   "cell_type": "code",
   "id": "312cfffd-e1fc-4fdb-bc42-dcfb99eb1bdd",
   "metadata": {
    "language": "python",
    "name": "rename_columns2"
   },
   "outputs": [],
   "source": "# order_history_mdf の列名を分かりやすく変更する\n# ****(columns={...}) で「元の列名 : 新しい列名」を指定する\n\nshipping_logs_mdf = shipping_logs_mdf.rename(columns = {\n    'Order ID': 'order_id',              # 注文ID → order_id\n    'Shipping Date': 'shipping_date',    # 発送日 → shipping_date\n    'Carrier': 'carrier',                # 配送業者 → carrier\n    'Tracking Number': 'trucking_number',# 追跡番号 → trucking_number\n    'Latitude': 'latitude',              # 緯度 → lattitude\n    'Longitude': 'longitude',            # 経度 → longitude\n    'Shipping Status': 'status'         # ステータス → status\n})\n\n# 列名が正しく変更されたかを確認する\nshipping_logs_mdf.columns",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ec8cd02-16c1-4999-8870-da25c1281f27",
   "metadata": {
    "language": "python",
    "name": "join_tables"
   },
   "outputs": [],
   "source": "# ---- 注文データと出荷データを結合 ----\n\n# order_history_mdf（注文データ）と shipping_logs_mdf（出荷データ）を\n# 'order_id' 列をキーにして結合（マージ）する\n\norder_shipping_mdf = spd.merge(\n    order_history_mdf,      # 左側のデータフレーム（注文履歴）\n    shipping_logs_mdf,      # 右側のデータフレーム（出荷ログ）\n    on='order_id',          # 結合キーとなる列\n    how='inner'             # 内部結合（両方に存在するデータのみ）\n)\n\n# 結合後のデータフレームの先頭5行を表示して確認\norder_shipping_mdf.head(5)\n\n\n# ✨ ポイント\n# --------------\n# spd.****() → pandas の **** と同じように使えるが、Snowpark pandas 上で動作する\n#\n# on='order_id' → 「注文ID」を使ってデータを紐付ける\n#\n# how='****'：\n#     両方のテーブルに存在する注文だけを残す\n#     片方だけにある注文は削除される\n#     結合すると、注文情報と出荷情報が 1行にまとめられる ので分析しやすくなる\n#         例) 「注文から出荷までの日数」や「商品ごとの売上と出荷状況」\n#\n# 外部結合（how='left' や how='right'）を使うと、片方にしかないデータも残せる\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1eb33443-f7c3-41a5-acfe-6cbe634ab990",
   "metadata": {
    "language": "python",
    "name": "product_order_counts"
   },
   "outputs": [],
   "source": "# ---- 商品ごとの注文件数を集計 ----\n\n# 'product_name' 列でグループ化して、注文件数を数える\n# ****() は各グループの行数（＝注文数）をカウントする\n# reset_index(name='order_count') で結果をデータフレーム形式に戻し、列名を 'order_count' に設定\nproduct_counts_mdf = order_shipping_mdf.groupby('product_name').size().reset_index(name='order_count')\n\n# ---- 注文件数の多い順に並べ替え ----\n\n# sort_values() で 'order_count' 列を降順（ascending=False）に並べる\nproduct_counts_mdf = product_counts_mdf.sort_values('order_count', ascending=False)\n\n# ---- 結果を表示 ----\nprint(\"\\nProduct Order Counts:\")\nst.dataframe(product_counts_mdf)\n\n\n\n# ✨ ポイント\n# --------------\n# ****('product_name') → 同じ商品ごとにまとめる \n#\n# .****() → 各商品の注文数をカウント\n#\n# .reset_index(name='order_count') → 集計結果をデータフレームとして整形し、列名を order_count に変更\n#\n# .sort_values(..., ascending=False) → 注文件数の多い順に並べ替える\n# --------------\n# これにより、どの商品が人気か（注文が多いか）が一目でわかる\n# もし上位5商品だけ見たい場合は product_counts_mdf.head(5) と書くと便利\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c92cd23f-deb0-41b3-91f5-7294a6ee6267",
   "metadata": {
    "name": "pivot",
    "collapsed": false
   },
   "source": "### 注文の配送ステータスごとにピボットする"
  },
  {
   "cell_type": "code",
   "id": "d803b11b-55e1-4b44-869f-e67d3c95639c",
   "metadata": {
    "language": "python",
    "name": "product_status_pivot"
   },
   "outputs": [],
   "source": "# ---- 商品ごとの注文ステータス別集計 ----\n\n# **** を使って集計\n# index='product_name' → 行に商品名を設定\n# columns='status' → 列に注文ステータス（例: shipped, pending, cancelled）を設定\n# values='order_id' → 注文IDを数える対象にする\n# aggfunc='count' → 各セルに注文数をカウント\n# fill_value=0 → データがない場合は 0 を埋める\nproduct_status_pivot_mdf = order_shipping_mdf.pivot_table(\n    index='product_name',\n    columns='status',\n    values='order_id',\n    aggfunc='count',\n    fill_value=0\n)\n\n# ---- 合計注文数の列を追加 ----\n\n# 行ごとの合計を計算して 'Total_Orders' 列として追加\nproduct_status_pivot_mdf['Total_Orders'] = product_status_pivot_mdf.sum(axis=1)\n\n# ---- 合計注文数の多い順に並べ替え ----\n\nproduct_status_pivot_mdf = product_status_pivot_mdf.sort_values('Total_Orders', ascending=False)\n\n# ---- 結果を表示 ----\nprint(\"\\nProduct Orders by Status:\")\nst.dataframe(product_status_pivot_mdf)\n\n# ✨ ポイント\n# --------------\n# **** → 行・列を指定して集計表（ピボットテーブル）を作る\n\n# aggfunc='count' → 注文数をカウント\n\n# sum(axis=1) → 行方向の合計を計算して「合計注文数」を追加\n\n# この表を作ると、\n# 商品ごとのステータス別の注文数が一目でわかる\n# 合計注文数で人気商品をすぐ把握できる",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62e8654c-416d-4279-8c21-b53987491ca8",
   "metadata": {
    "name": "intro2",
    "collapsed": false
   },
   "source": "## Avalanche社は、各製品に対する顧客レビューについても理解したいと考えています。  \n\n![IMAGE](https://lh3.googleusercontent.com/pw/AP1GczMuP-pHWhjNDtQwRpMYm0FKey9xlDfRMvcSa6HhxnJrhG-oCs6ydlOhpCvR5VcNDjbFNRir_H4XsFaay-lehwzRV1pgKoB9DjJ31SduUCD2F1gwmZgG4SAM6vNseULS3tYZoW7taYzTW-gc5Lt-4gu3=w960-h540-s-no-gm?authuser=0)\n\n\n\nこの分析を [Snowpark DataFrame API](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes) を使って実行してみましょう。"
  },
  {
   "cell_type": "code",
   "id": "700a31fe-3a77-4966-bfef-c29cb3c1fe1a",
   "metadata": {
    "language": "sql",
    "name": "create_snowflake_objects"
   },
   "outputs": [],
   "source": "-- ---- データベースとスキーマの作成（Snowsight UI で実行する場合） ----\n-- CREATE OR REPLACE DATABASE avalanche_db;\n-- CREATE OR REPLACE SCHEMA avalanche_schema;\n\n-- 既存データベースを使用する\nUSE DATABASE avalanche_db;\n\n-- 既存スキーマを使用する\nUSE SCHEMA avalanche_schema;\n\n\n-- ---- ファイルを格納するステージ（Stage）の作成 ----\n-- Stage とは、Snowflake にデータを取り込む前に一時的にファイルを置いておく場所です\nCREATE OR REPLACE STAGE avalanche_stage\n  URL = 's3://sfquickstarts/misc/avalanche/csv/'  -- S3 バケットの場所を指定\n  DIRECTORY = (ENABLE = TRUE AUTO_REFRESH = TRUE); -- ディレクトリ構造を有効化、自動更新ON\n\n-- Stage 内のファイル一覧を確認\nls @avalanche_stage;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0f6a184-8c2f-4d54-a606-fd3f2acc1d22",
   "metadata": {
    "name": "load_reviews",
    "collapsed": false
   },
   "source": "### 顧客レビューを Snowflake のテーブルに読み込む"
  },
  {
   "cell_type": "code",
   "id": "4903c319-f55a-4773-8e58-daaec82f72d3",
   "metadata": {
    "language": "sql",
    "name": "create_customer_reviews_table"
   },
   "outputs": [],
   "source": "-- ---- テーブルの作成 ----\n-- customer_reviews という名前のテーブルを作成\n-- 商品名、レビュー日、レビュー本文、感情スコアを保存する\nCREATE OR REPLACE TABLE customer_reviews (\n    product VARCHAR,          -- 商品名（文字列）\n    date DATE,                -- レビュー日（DATE型）\n    summary TEXT,             -- レビュー本文（TEXT型）\n    sentiment_score FLOAT     -- 感情スコア（数値、小数点）\n);\n\n\n-- ---- CSV ファイルからデータをテーブルにロード ----\nCOPY INTO customer_reviews\nFROM @avalanche_stage/customer_reviews.csv   -- 先ほど作成した Stage 内の CSV を指定\nFILE_FORMAT = (\n    TYPE = CSV,                               -- CSV形式のファイル\n    FIELD_DELIMITER = ',',                     -- カラム区切り文字はカンマ\n    SKIP_HEADER = 1,                           -- 1行目はヘッダーなのでスキップ\n    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',       -- 値が \" \" で囲まれている場合に対応\n    TRIM_SPACE = TRUE,                         -- 前後の空白を削除\n    NULL_IF = ('NULL', 'null'),               -- \"NULL\" または \"null\" は NULL として扱う\n    EMPTY_FIELD_AS_NULL = TRUE                -- 空欄も NULL として扱う\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d558d8e0-c499-4ae5-9ca0-85497fedc71a",
   "metadata": {
    "language": "python",
    "name": "load_customer_reviews"
   },
   "outputs": [],
   "source": "# ---- Snowflake テーブルを Snowpark DataFrame として読み込む ----\n\n# 'customer_reviews' テーブルを Snowpark DataFrame として取得\ncustomer_reviews_sdf = session.table('customer_reviews')\n\n# 取得した Snowpark DataFrame の内容を確認\ncustomer_reviews_sdf\n\n\n# ✨ ポイント\n# --------------\n# session.table('table_name')\n#   Snowflake 上の既存テーブルを Snowpark DataFrame として扱う\n#   pandas の DataFrame に似ているが、実際のデータは Snowflake にあり、クエリ実行時に必要な部分だけ取得する\n#\n# customer_reviews_sdf\n#   この変数に Snowpark DataFrame が格納される\n#   データの操作（フィルタリング、集計、結合など）を Snowflake 側で効率的に実行可能\n\n# 💡 補足\n# --------------\n# Snowpark DataFrame は Lazy Evaluation（遅延評価） です\n#   customer_reviews_sdf を定義しただけではまだデータは取得されない\n#   データを確認したい場合は .show() や .to_pandas() などで明示的に取得する必要があります",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3eddc5a-cd65-4ceb-a20e-057eade19152",
   "metadata": {
    "language": "python",
    "name": "drop_sentiment"
   },
   "outputs": [],
   "source": "product_sentiment_sdf = customer_reviews_sdf.group_by('PRODUCT') \\\n    .agg(F.round(F.avg('SENTIMENT_SCORE'),2).alias('AVG_SENTIMENT_SCORE')) \\\n    .sort(F.col('AVG_SENTIMENT_SCORE').desc())\n\n# Display the results\nprint(\"\\nAverage Sentiment Scores by Product:\")\nproduct_sentiment_sdf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c125cda9-34ed-421b-ab71-b92b5ba546a4",
   "metadata": {
    "name": "visualize_md",
    "collapsed": false
   },
   "source": "## 📊 データの可視化\n\n[Altair](https://altair-viz.github.io/)を使用して、データ分布をヒストグラムとして簡単に可視化できます。"
  },
  {
   "cell_type": "code",
   "id": "0a01df47-4480-4069-adca-954fb3bb8fc0",
   "metadata": {
    "language": "python",
    "name": "visualize"
   },
   "outputs": [],
   "source": "import altair as alt\n\n# 追加したパッケージ\nimport matplotlib.pyplot as plt\n\npdf = customer_reviews_sdf.to_pandas()\nchart = alt.Chart(pdf, title='評価分布').mark_bar().encode(\n    alt.X(\"SENTIMENT_SCORE\", bin=alt.Bin(step=0.5)),\n    y='count()'\n)\n\nst.altair_chart(chart)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "987baa63-1214-40f2-b323-fdf963c63876",
   "metadata": {
    "name": "plotting_md",
    "collapsed": false
   },
   "source": "チャートをカスタマイズして、カーネル密度推定（KDE）と中央値をプロットしたいとします。matplotlibを使用して価格分布をプロットできます。`.plot`コマンドは内部的に`scipy`を使用してKDEプロファイルを計算することに注意してください。これは、このチュートリアルの前半でパッケージとして追加したものです。"
  },
  {
   "cell_type": "code",
   "id": "76f6993a-e1eb-49b1-bfab-f32cc9350eb9",
   "metadata": {
    "language": "python",
    "name": "plotting"
   },
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize = (6,3))\nplt.tick_params(left = False, right = False , labelleft = False) \n\nprice = order_history_mdf[\"price\"]\nprice.plot(kind = \"hist\", density = True, bins = 15)\nprice.plot(kind=\"kde\", color='#c44e52')\n\n\n# パーセンタイルを計算\nmedian = price.median()\nax.axvline(median,0, color='#dd8452', ls='--')\nax.text(median,0.8, f'Median: {median:.2f}  ',\n        ha='right', va='center', color='#dd8452', transform=ax.get_xaxis_transform())\n\n# チャートを美しくする\nplt.style.use(\"bmh\")\nplt.title(\"Price Distribution\")\nplt.xlabel(\"Price (Binned)\")\nleft, right = plt.xlim()   \nplt.xlim((0, right))  \n# 目盛りと軸線を削除\nax.tick_params(left = False, bottom = False)\nfor ax, spine in ax.spines.items():\n    spine.set_visible(False)\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1fa1fbd-5483-4e60-a893-f92fab942ad6",
   "metadata": {
    "name": "cell_reference_md",
    "collapsed": false
   },
   "source": "## サブクエリ/セル間の参照\n\nセルに名前をつけて、後続のセルでその出力を参照することができます\n"
  },
  {
   "cell_type": "markdown",
   "id": "fca8bf3e-25a1-458d-8f59-169f4a1ec220",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "Jinjaを利用して別のSQLセルからSQLテーブルを参照することで、CTEを簡素化することができます。\n\n```sql\nSELECT * FROM {{cell}}\n```"
  },
  {
   "cell_type": "code",
   "id": "15afa98b-8abf-4888-9029-cf197cc05393",
   "metadata": {
    "language": "sql",
    "name": "subqueries"
   },
   "outputs": [],
   "source": "select \n    product, \n    avg(sentiment_score) as avg_score, \n    min(sentiment_score) as min_score, \n    max(sentiment_score) as max_score\nfrom customer_reviews\ngroup by all;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "425d7660-8ecc-4b3e-89e3-619907a8d966",
   "metadata": {
    "language": "sql",
    "name": "subqueries2"
   },
   "outputs": [],
   "source": "select * from {{subqueries}}\nWHERE avg_score > 0.5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77fde296-4fa7-49bf-add4-79cafe529d48",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "SQL結果にPythonから直接アクセスし、結果をpandas DataFrameに変換できます。🐼\n\n```python\n# SQLセルの出力をSnowpark DataFrameとしてアクセス\nmy_snowpark_df = sql_querying.to_df()\n``` \n\n```python\n# SQLセルの出力をpandas DataFrameに変換\nmy_df = sql_querying.to_pandas()\n``` "
  },
  {
   "cell_type": "code",
   "id": "e4677009-d9e0-4690-82ab-8a7196026fae",
   "metadata": {
    "language": "python",
    "name": "cell_reference"
   },
   "outputs": [],
   "source": "my_df = subqueries2.to_df()\nmy_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce58c54d-15fa-4fba-a124-1a56a7947b29",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## ステージパッケージの追加\n使用したいPythonパッケージがAnacondaで利用できない場合は、パッケージをステージにアップロードし、ステージからインポートすることができます。ここでは、カスタムパッケージをノートブックにインポートする簡単な例を示します。"
  },
  {
   "cell_type": "code",
   "id": "c1d2e372-2354-48c9-8a27-f388ab75ffef",
   "metadata": {
    "language": "sql",
    "name": "git_integration"
   },
   "outputs": [],
   "source": "-- ステージの作成\nCREATE OR REPLACE STAGE AVALANCHE_DB.AVALANCHE_SCHEMA.FILE DIRECTORY = (ENABLE = TRUE);\n\n// Step3: 公開されているGitからスクリプトを取得 //\n-- Git連携のため、API統合を作成する\nCREATE OR REPLACE API INTEGRATION git_api_integration\n  API_PROVIDER = git_https_api\n  API_ALLOWED_PREFIXES = ('https://github.com/sfc-gh-skawakami/')\n  ENABLED = TRUE;\n\n-- GIT統合の作成\nCREATE OR REPLACE GIT REPOSITORY GIT_INTEGRATION_FOR_HANDSON\n  API_INTEGRATION = git_api_integration\n  ORIGIN = 'https://github.com/sfc-gh-skawakami/sfc-jp-notebook_de_101.git';\n\n\nALTER GIT REPOSITORY GIT_INTEGRATION_FOR_HANDSON FETCH;\n\n-- チェックする\nls @GIT_INTEGRATION_FOR_HANDSON/branches/main;\n\n-- Githubからファイルを持ってくる\nCOPY FILES INTO @AVALANCHE_DB.AVALANCHE_SCHEMA.FILE FROM @GIT_INTEGRATION_FOR_HANDSON/branches/main/simple.zip;\nls @AVALANCHE_DB.AVALANCHE_SCHEMA.FILE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f89368f4-1053-481b-a446-d4e1874b8f83",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "`simple.zip`の内容\n\nsimple/__init__.py\n\n```python\nimport streamlit as st\n\ndef greeting():\n  return \"Hello world!\"\n\ndef hi():\n  st.write(greeting())\n```"
  },
  {
   "cell_type": "code",
   "id": "7c601241-1133-4280-9486-9527a480629b",
   "metadata": {
    "language": "python",
    "name": "stage_packages"
   },
   "outputs": [],
   "source": "import simple\n\nsimple.hi()",
   "execution_count": null
  }
 ]
}